#! /usr/bin/env python3

import tf
from hri_msgs.msg import IdsList, FacialLandmarks, PointOfInterest2D
from sensor_msgs.msg import Image, CameraInfo, RegionOfInterest
from std_msgs.msg import Empty
from cv_bridge import CvBridge
import rospy
import math
import cv2
import numpy as np
import uuid
from functools import partial

import mediapipe as mp

mp_face_detection = mp.solutions.face_detection


# nb of pixels between the centers of to successive regions of interest to
# consider they belong to the same person
MAX_ROIS_DISTANCE = 20

# max scale factor between two successive regions of interest to consider they
# belong to the same person
MAX_SCALING_ROIS = 1.2

# default size in pixels for the re-published faces
# can be changed via the ROS parameters /humans/faces/width and /humans/faces/height
cropped_face_width = 128
cropped_face_height = 128

# face key points
# These use a different reference frame maybe?
P3D_RIGHT_EYE = (-20., -65.5,-5.)
P3D_LEFT_EYE = (-20., 65.5,-5.)
P3D_RIGHT_EAR = (-100., -77.5,-6.)
P3D_LEFT_EAR = (-100., 77.5,-6.)
P3D_NOSE = (21.0, 0., -48.0)
P3D_STOMION = (10.0, 0., -75.0)

points_3D = np.array([
    P3D_NOSE,       
    P3D_RIGHT_EYE,
    P3D_LEFT_EYE,
    P3D_STOMION,
    P3D_RIGHT_EAR,
    P3D_LEFT_EAR]
) # ==> No need to do this every time, can just create it
  # as a global array

def normalized_to_pixel_coordinates(
    normalized_x: float,
    normalized_y: float,
    image_width: int,
    image_height: int,
):

    x_px = min(math.floor(normalized_x * image_width), image_width - 1)
    y_px = min(math.floor(normalized_y * image_height), image_height - 1)
    return x_px, y_px


class FaceDetector:
    def __init__(self):

        self.detector = mp_face_detection.FaceDetection(
            model_selection=1, min_detection_confidence=0.5
        )

    def detect(self, img):
        """img is expected as RGB"""
        img_rows, img_cols, _ = img.shape

        detections = self.detector.process(img).detections
        results = []
        results.append(self.get_boundingboxes(
            detections, img_cols, img_rows))

        nose_tips = []
        right_eyes = []
        left_eyes = []
        mouth_centers = []
        right_ear_tragions = []
        left_ear_tragions = []

        if detections:
            for detection in detections:

                nose_tip = mp_face_detection.get_key_point(detection, 
                    mp_face_detection.FaceKeyPoint.NOSE_TIP)
                nose_tips.append([nose_tip.x, nose_tip.y])
                right_eye = mp_face_detection.get_key_point(detection, 
                    mp_face_detection.FaceKeyPoint.RIGHT_EYE)
                right_eyes.append([right_eye.x, right_eye.y])
                left_eye = mp_face_detection.get_key_point(detection, 
                    mp_face_detection.FaceKeyPoint.LEFT_EYE)
                left_eyes.append([left_eye.x, left_eye.y])
                mouth_center = mp_face_detection.get_key_point(detection, 
                    mp_face_detection.FaceKeyPoint.MOUTH_CENTER)
                mouth_centers.append([mouth_center.x, mouth_center.y])
                right_ear_tragion = mp_face_detection.get_key_point(detection, 
                    mp_face_detection.FaceKeyPoint.RIGHT_EAR_TRAGION)             
                right_ear_tragions.append([right_ear_tragion.x, right_ear_tragion.y])
                left_ear_tragion = mp_face_detection.get_key_point(detection, 
                    mp_face_detection.FaceKeyPoint.LEFT_EAR_TRAGION)             
                left_ear_tragions.append([left_ear_tragion.x, left_ear_tragion.y])

        results.append(nose_tips)
        results.append(right_eyes)
        results.append(left_eyes)
        results.append(mouth_centers)
        results.append(right_ear_tragions)
        results.append(left_ear_tragions)

        return results

    def get_boundingboxes(self, detections, image_cols, image_rows):
        """
        Based on https://github.com/google/mediapipe/blob/master/mediapipe/python/solutions/drawing_utils.py
        """
        res = []
        if not detections:
            return res

        for detection in detections:
            bb = detection.location_data.relative_bounding_box
            x, y = normalized_to_pixel_coordinates(
                bb.xmin, bb.ymin, image_cols, image_rows
            )
            w, h = normalized_to_pixel_coordinates(
                bb.width, bb.height, image_cols, image_rows
            )

            res.append((x, y, w, h))

        return res

    def __str__(self):
        return "Google mediapipe face detector"


class RosFaceDetector:
    def __init__(self, debug=False, preallocate=False):

        self.debug = debug

        semaphore_pub = rospy.Publisher(
            "/hri_face_detect/ready", Empty, queue_size=1, latch=True
        )
        self.faces_pub = rospy.Publisher(
            "/humans/faces/tracked", IdsList, queue_size=1)

        self.facedetector = FaceDetector()
        self.image_sub = rospy.Subscriber(
            "image", Image, partial(self.callback, preallocate)
        )
        self.image_info_sub = rospy.Subscriber(
            "camera_info", CameraInfo, self.info_callback)

        self.debug_image_publisher = rospy.Publisher(
            "/debug_head_pose_image", Image, queue_size=1)

        # holds all the individual face publishers -- only used when pre-allocating face publishers
        if preallocate:
            self.face_pubs = {
                "%05d"
                % i: (
                    rospy.Publisher(
                        "/humans/faces/%05d/roi" % i, RegionOfInterest, queue_size=1
                    ),
                    rospy.Publisher(
                        "/humans/faces/%05d/cropped" % i, Image, queue_size=1
                    ),
                )
                for i in range(1, 30)
            }
        else:
            self.face_pubs = []

        rospy.loginfo(
            "Ready. Waiting for images to be published on %s." % self.image_sub.name
        )
        semaphore_pub.publish(Empty())

        # last-used face id
        self.last_id = 1

        # ID -> (publisher, RoI, nb_frames_visible)
        self.detectedFaces = {}

        self.tb = tf.TransformBroadcaster()

    def find_previous_match(self, bb):
        for id, value in self.detectedFaces.items():
            _, prev_bb, _, _ = value
            if (
                self.distance_rois(
                    prev_bb, bb) < MAX_ROIS_DISTANCE * MAX_ROIS_DISTANCE
                and 1 / MAX_SCALING_ROIS < prev_bb.width / bb.width < MAX_SCALING_ROIS
                and 1 / MAX_SCALING_ROIS < prev_bb.height / bb.height < MAX_SCALING_ROIS
            ):
                return id
        return None

    def publish_cropped_faces(self, src_image, bbs_and_publishers):

        for id, kv in bbs_and_publishers.items():
            pubs, bb, _, _= kv

            if not pubs:
                continue

            # no-one interested in the face image? skip it!
            if pubs[1].get_num_connections() == 0 and not self.debug:
                continue

            roi = src_image[
                bb.y_offset: bb.y_offset + bb.height,
                bb.x_offset: bb.x_offset + bb.width,
            ]

            sx = cropped_face_width * 1.0 / bb.width
            sy = cropped_face_height * 1.0 / bb.height

            scale = min(sx, sy)

            scaled = cv2.resize(roi, None, fx=scale, fy=scale)
            scaled_h, scaled_w = scaled.shape[:2]

            output = np.zeros(
                (cropped_face_width, cropped_face_height, 3), np.uint8)

            x_offset = int((cropped_face_width - scaled_w) / 2)
            y_offset = int((cropped_face_height - scaled_h) / 2)

            output[
                y_offset: y_offset + scaled_h, x_offset: x_offset + scaled_w
            ] = scaled

            if self.debug:

                cv2.imshow("Face %s" % id, output)

            pubs[1].publish(CvBridge().cv2_to_imgmsg(output, encoding="bgr8"))

    def info_callback(self, cameraInfo):

        if not hasattr(self, 'cameraInfo'):
            self.cameraInfo = cameraInfo

            self.K = np.zeros((3, 3), np.float32)
            self.K[0][0:3] = self.cameraInfo.K[0:3]
            self.K[1][0:3] = self.cameraInfo.K[3:6]
            self.K[2][0:3] = self.cameraInfo.K[6:9]
            print(self.K)

    def callback(self, preallocate, rgb_msg):

        image = CvBridge().imgmsg_to_cv2(rgb_msg, desired_encoding="bgr8")
        img_height, img_width, _ = image.shape

        detections = self.facedetector.detect(image)
        bbs = detections[0]
        nose_tips = detections[1]
        right_eyes = detections[2]
        left_eyes = detections[3]
        mouth_centers = detections[4]
        right_ear_tragions = detections[5]
        left_ear_tragions = detections[6]

        currentFaces = {}
        for i, bb in enumerate(bbs):
            x, y, w, h = bb
            bb = RegionOfInterest(
                max(0, x),
                max(0, y),
                min(img_height - y, h),
                min(img_width - x, w),
                True,
            )

            landmarks = FacialLandmarks(
                [PointOfInterest2D(0, 0, 1) for _ in range(70)], 
                img_height, 
                img_width)

            # Setting the landmarks. The values provided by the 
            # MediaPipe estimatore used here are mapped according
            # to the ROS4HRI facial landmarks convention

            landmarks.landmarks[FacialLandmarks.NOSE].x = nose_tips[i][0]
            landmarks.landmarks[FacialLandmarks.NOSE].y = nose_tips[i][1]

            landmarks.landmarks[FacialLandmarks.RIGHT_PUPIL].x = right_eyes[i][0]
            landmarks.landmarks[FacialLandmarks.RIGHT_PUPIL].y = right_eyes[i][1]

            landmarks.landmarks[FacialLandmarks.LEFT_PUPIL].x = left_eyes[i][0]
            landmarks.landmarks[FacialLandmarks.LEFT_PUPIL].y = left_eyes[i][1]

            landmarks.landmarks[FacialLandmarks.RIGHT_EAR].x = right_ear_tragions[i][0]
            landmarks.landmarks[FacialLandmarks.RIGHT_EAR].y = right_ear_tragions[i][1]

            landmarks.landmarks[FacialLandmarks.LEFT_EAR].x = left_ear_tragions[i][0]
            landmarks.landmarks[FacialLandmarks.LEFT_EAR].y = left_ear_tragions[i][1]

            landmarks.landmarks[FacialLandmarks.MOUTH_INNER_TOP_2].x = mouth_centers[i][0]
            landmarks.landmarks[FacialLandmarks.MOUTH_INNER_TOP_2].y = mouth_centers[i][1]

            id = self.find_previous_match(bb)
            if id:
                # we re-detect a face: if it is a 2nd frame, we create a publisher for it.
                if not self.detectedFaces[id][0]:
                    want_deterministic_id = preallocate
                    final_id = self.generate_face_id(want_deterministic_id)
                    rospy.loginfo("New face [face_%s]" % final_id)

                    if preallocate:
                        pubs = self.face_pubs[final_id]
                    else:
                        pubs = (
                            rospy.Publisher(
                                "/humans/faces/%s/roi" % final_id,
                                RegionOfInterest,
                                queue_size=1,
                            ),
                            rospy.Publisher(
                                "/humans/faces/%s/cropped" % final_id,
                                Image,
                                queue_size=1,
                            ),
                            rospy.Publisher("/humans/faces/%s/landmarks" % final_id,
                                FacialLandmarks,
                                queue_size=1,
                            )
                        )

                    currentFaces[final_id] = (
                        pubs, bb, self.detectedFaces[id][2] + 1, landmarks)
                else:
                    currentFaces[id] = (
                        self.detectedFaces[id][0],
                        bb,
                        self.detectedFaces[id][2] + 1,
                        landmarks
                    )
                    ### Face pose estimation ###
                    if hasattr(self, 'K'):
                        points_2D = np.array([
                            normalized_to_pixel_coordinates(
                                nose_tips[i][0],
                                nose_tips[i][1],
                                img_width,
                                img_height),
                            normalized_to_pixel_coordinates(
                                right_eyes[i][0],
                                right_eyes[i][1],
                                img_width,
                                img_height),
                            normalized_to_pixel_coordinates(
                                left_eyes[i][0],
                                left_eyes[i][1],
                                img_width,
                                img_height),
                            normalized_to_pixel_coordinates(
                                mouth_centers[i][0],
                                mouth_centers[i][1],
                                img_width,
                                img_height),
                            normalized_to_pixel_coordinates(
                                right_ear_tragions[i][0],
                                right_ear_tragions[i][1],
                                img_width,
                                img_height),
                            normalized_to_pixel_coordinates(
                                left_ear_tragions[i][0],
                                left_ear_tragions[i][1],
                                img_width,
                                img_height)
                            ], dtype="double")

                        success, rotation_vector, translation_vector = cv2.solvePnP(
                                points_3D, 
                                points_2D, 
                                self.K, 
                                None,
                                tvec=np.array([0., 0., 1000.]),
                                rvec=np.array([1., 0., 0.]) ,
                                flags=0,
                                useExtrinsicGuess=False)
                        
                        theta = np.sqrt(
                            rotation_vector[0]**2 \
                            + rotation_vector[1]**2 \
                            + rotation_vector[2]**2)
                        
                        a = np.cos(theta/2)
                        b = rotation_vector[0]*np.sin(theta/2)/theta
                        c = rotation_vector[1]*np.sin(theta/2)/theta
                        d = rotation_vector[2]*np.sin(theta/2)/theta

                        norm = a**2 + b**2 + c**2 + d**2
                        a_inv = a/norm
                        b_inv = -b/norm 
                        c_inv = -c/norm 
                        d_inv = -d/norm

                        self.tb.sendTransform(
                            (-translation_vector[0]/1000, \
                             -translation_vector[1]/1000, \
                             -translation_vector[2]/1000),
                            (a, b, c, d),
                            rospy.Time.now(),
                            "face"+id,
                            "camera_color_optical_frame")

                    #######################
            else:
                # we 'provisionally' store the face - we'll create a publisher and start publishing only if we see the
                # face a second time
                id = self.generate_tmp_face_id()
                currentFaces[id] = (None, bb, 1, landmarks)

        # iterate over faces not seen anymore, and unregister corresponding publishers
        for id, value in self.detectedFaces.items():
            if id not in currentFaces:
                pubs, _, nb_frames, _ = value
                if pubs:
                    rospy.loginfo(
                        "Face [face_%s] lost. It remained visible for %s frames"
                        % (id, nb_frames)
                    )
                    for pub in pubs:
                        pub.unregister()

        self.detectedFaces = currentFaces

        list_ids = []

        for id, value in self.detectedFaces.items():
            pubs, bb, _, landmarks = value
            if pubs:
                list_ids.append(str(id))
                pubs[0].publish(bb)
                pubs[2].publish(landmarks)

        self.publish_cropped_faces(image, self.detectedFaces)
        self.faces_pub.publish(IdsList(rgb_msg.header, list_ids))

        if self.debug:
            # Draw the face detection annotations on the image.
            image.flags.writeable = True
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
            rospy.loginfo("%s faces detected" % len(self.detectedFaces))
            for id, value in self.detectedFaces.items():
                pub, bb, nb_frames = value
                if not pub:
                    continue
                cv2.rectangle(
                    image,
                    (bb.x_offset, bb.y_offset),
                    (bb.x_offset + bb.width, bb.y_offset + bb.height),
                    (255, 255, 0),
                    2,
                )
                cv2.putText(
                    image,
                    id,
                    (bb.x_offset, bb.y_offset),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1,
                    (255, 0, 0),
                )

            cv2.imshow("MediaPipe Face Detection", image)
            cv2.waitKey(5)

    def distance_rois(self, bb1, bb2):
        x1, y1 = bb1.x_offset + bb1.width / 2, bb1.y_offset + bb1.height / 2
        x2, y2 = bb2.x_offset + bb2.width / 2, bb2.y_offset + bb2.height / 2

        return (x1 - x2) * (x1 - x2) + (y1 - y2) * (y1 - y2)

    def generate_face_id(self, deterministic=False):
        if deterministic:
            id_str = "%05d" % self.last_id
            self.last_id = (self.last_id + 1) % 10000
            return id_str

        else:
            return str(uuid.uuid4())[:5]  # for a 5 char long ID

    def generate_tmp_face_id(self):
        return str(uuid.uuid4())[:5]  # for a 5 char long ID


if __name__ == "__main__":
    rospy.init_node("hri_face_detect")

    cropped_face_width = rospy.get_param(
        "/humans/faces/width", cropped_face_width)
    cropped_face_height = rospy.get_param(
        "/humans/faces/height", cropped_face_height)

    debug = rospy.get_param("~debug", False)

    # if set to true, face IDs will be generated as a sequence of integers,
    # starting at 00001, and ROS publishers for each face are created *before*
    # the faces are detected. This makes it possible for client to subscribe
    # early to these topics, to avoid missing any message. This is useful to eg
    # annotate datasets.

    # if false, face IDs will be a random set of 5 characters in [0-9a-f], and
    # face subtopics will be created on the fly.
    preallocate = rospy.get_param("~preallocate_topics", False)

    detector = RosFaceDetector(debug, preallocate)

    rospy.spin()
