#! /usr/bin/env python3

import tf
from hri_msgs.msg import IdsList, FacialLandmarks, PointOfInterest2D
from sensor_msgs.msg import Image, CameraInfo, RegionOfInterest
from std_msgs.msg import Empty, Header
import tf_conversions.posemath as pm
from cv_bridge import CvBridge
import rospy
import math
import cv2
import numpy as np
import uuid
from functools import partial
from tf.transformations import quaternion_from_euler

import mediapipe as mp

mp_face_detection = mp.solutions.face_detection
mp_face_mesh = mp.solutions.face_mesh

# nb of pixels between the centers of
# two successive regions of interest to
# consider they belong to the same person
MAX_ROIS_DISTANCE = 20

# max scale factor between two successive
# regions of interest to consider they
# belong to the same person
MAX_SCALING_ROIS = 1.2

# default size in pixels for the re-published faces
# can be changed via the ROS parameters
# /humans/faces/width and /humans/faces/height
cropped_face_width = 128
cropped_face_height = 128

# face key points
P3D_RIGHT_EYE = (-20., -65.5, -5.)
P3D_LEFT_EYE = (-20., 65.5, -5.)
P3D_RIGHT_EAR = (-100., -77.5, -6.)
P3D_LEFT_EAR = (-100., 77.5, -6.)
P3D_NOSE = (21.0, 0., -48.0)
P3D_STOMION = (10.0, 0., -75.0)


points_3D = np.array([
    P3D_NOSE,
    P3D_RIGHT_EYE,
    P3D_LEFT_EYE,
    P3D_STOMION,
    P3D_RIGHT_EAR,
    P3D_LEFT_EAR]
)

# ros4hri to mediapipe mapping
"""
ros4hri facial landmarks ref: https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/keypoints_face.png
mediapipe face mesh landmarks ref: https://i.stack.imgur.com/5Mohl.jpg 
"""
ros4hri_to_mediapipe = [None] * 68
# The ROS4HRI FacialLandmarks message defines 70 landmarks,
# however Mediapipe Face Mesh estimator does not provide
# an estimation for the pupils position ==> 70 - 2 = 68

ros4hri_to_mediapipe[FacialLandmarks.RIGHT_EAR] = 34
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_PROFILE_1] = 227
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_PROFILE_2] = 137
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_PROFILE_3] = 177
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_PROFILE_4] = 215
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_PROFILE_5] = 135
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_PROFILE_6] = 170
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_PROFILE_7] = 171
ros4hri_to_mediapipe[FacialLandmarks.MENTON] = 175
ros4hri_to_mediapipe[FacialLandmarks.LEFT_EAR] = 264
ros4hri_to_mediapipe[FacialLandmarks.LEFT_PROFILE_1] = 447
ros4hri_to_mediapipe[FacialLandmarks.LEFT_PROFILE_2] = 366
ros4hri_to_mediapipe[FacialLandmarks.LEFT_PROFILE_3] = 401
ros4hri_to_mediapipe[FacialLandmarks.LEFT_PROFILE_4] = 435
ros4hri_to_mediapipe[FacialLandmarks.LEFT_PROFILE_5] = 364
ros4hri_to_mediapipe[FacialLandmarks.LEFT_PROFILE_6] = 395
ros4hri_to_mediapipe[FacialLandmarks.LEFT_PROFILE_7] = 396
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_EYEBROW_OUTSIDE] = 70
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_EYEBROW_1] = 63
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_EYEBROW_2] = 105
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_EYEBROW_3] = 66
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_EYEBROW_INSIDE] = 107
ros4hri_to_mediapipe[FacialLandmarks.LEFT_EYEBROW_OUTSIDE] = 300
ros4hri_to_mediapipe[FacialLandmarks.LEFT_EYEBROW_1] = 293
ros4hri_to_mediapipe[FacialLandmarks.LEFT_EYEBROW_2] = 334
ros4hri_to_mediapipe[FacialLandmarks.LEFT_EYEBROW_3] = 296
ros4hri_to_mediapipe[FacialLandmarks.LEFT_EYEBROW_INSIDE] = 336
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_EYE_OUTSIDE] = 130
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_EYE_TOP_1] = 29
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_EYE_TOP_2] = 28
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_EYE_INSIDE] = 243
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_EYE_BOTTOM_1] = 24
ros4hri_to_mediapipe[FacialLandmarks.RIGHT_EYE_BOTTOM_2] = 22
ros4hri_to_mediapipe[FacialLandmarks.LEFT_EYE_OUTSIDE] = 359
ros4hri_to_mediapipe[FacialLandmarks.LEFT_EYE_TOP_1] = 259
ros4hri_to_mediapipe[FacialLandmarks.LEFT_EYE_TOP_2] = 258
ros4hri_to_mediapipe[FacialLandmarks.LEFT_EYE_INSIDE] = 463
ros4hri_to_mediapipe[FacialLandmarks.LEFT_EYE_BOTTOM_1] = 254
ros4hri_to_mediapipe[FacialLandmarks.LEFT_EYE_BOTTOM_2] = 252
ros4hri_to_mediapipe[FacialLandmarks.SELLION] = 6
ros4hri_to_mediapipe[FacialLandmarks.NOSE_1] = 197
ros4hri_to_mediapipe[FacialLandmarks.NOSE_2] = 4
ros4hri_to_mediapipe[FacialLandmarks.NOSE] = 1
ros4hri_to_mediapipe[FacialLandmarks.NOSTRIL_1] = 242
ros4hri_to_mediapipe[FacialLandmarks.NOSTRIL_2] = 141
ros4hri_to_mediapipe[FacialLandmarks.NOSTRIL_3] = 94
ros4hri_to_mediapipe[FacialLandmarks.NOSTRIL_4] = 370
ros4hri_to_mediapipe[FacialLandmarks.NOSTRIL_5] = 462
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_OUTER_RIGHT] = 61
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_OUTER_TOP_1] = 40
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_OUTER_TOP_2] = 37
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_OUTER_TOP_3] = 0
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_OUTER_TOP_4] = 267
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_OUTER_TOP_5] = 270
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_OUTER_LEFT] = 291
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_OUTER_BOTTOM_1] = 321
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_OUTER_BOTTOM_2] = 314
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_OUTER_BOTTOM_3] = 17
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_OUTER_BOTTOM_4] = 84
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_OUTER_BOTTOM_5] = 91
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_INNER_RIGHT] = 62
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_INNER_TOP_1] = 41
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_INNER_TOP_2] = 12
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_INNER_TOP_3] = 271
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_INNER_LEFT] = 292
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_INNER_BOTTOM_1] = 403
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_INNER_BOTTOM_2] = 15
ros4hri_to_mediapipe[FacialLandmarks.MOUTH_INNER_BOTTOM_3] = 179


def normalized_to_pixel_coordinates(
    normalized_x: float,
    normalized_y: float,
    image_width: int,
    image_height: int,
):

    x_px = min(math.floor(normalized_x * image_width), image_width - 1)
    y_px = min(math.floor(normalized_y * image_height), image_height - 1)
    return x_px, y_px


class FaceDetector:
    def __init__(self, face_mesh=True, max_num_faces=10):

        self.face_mesh = face_mesh

        if not face_mesh:
            self.detector = mp_face_detection.FaceDetection(
                model_selection=1, min_detection_confidence=0.5
            )
        else:
            self.detector = mp_face_mesh.FaceMesh(
                static_image_mode=False,
                max_num_faces=max_num_faces,
                refine_landmarks=True,
                min_detection_confidence=0.5)

    def make_facial_landmarks_msg(self, detection, img_height, img_width):

        # PoI = Points of Interest
        poi = [PointOfInterest2D(
                int(detection.landmark[ros4hri_to_mediapipe[idx]].x*img_width),
                int(detection.landmark[ros4hri_to_mediapipe[idx]].y*img_height),
                1) for idx in range(68)]

        # the last two facial landmarks represent the pupils
        # and Mediapipe does not provide pupils estimation.
        poi.append(PointOfInterest2D(0, 0, 1)) # RIGHT_PUPIL 
        poi.append(PointOfInterest2D(0, 0, 1)) # LEFT_PUPIL

        landmarks = FacialLandmarks(
            poi,
            img_height,
            img_width)

        return landmarks

    def detect(self, img, img_width, img_height):
        """img is expected as RGB"""
        img_rows, img_cols, _ = img.shape

        detections = self.detector.process(img).detections
        results = []
        results.append(self.get_boundingboxes(
            detections, img_cols, img_rows))

        nose_tips = []
        right_eyes = []
        left_eyes = []
        mouth_centers = []
        right_ear_tragions = []
        left_ear_tragions = []
        facial_landmarks_msg = []

        if detections:
            for detection in detections:

                landmarks = FacialLandmarks(
                    [PointOfInterest2D(0, 0, 1) for _ in range(70)],
                    img_height,
                    img_width)
                nose_tip = mp_face_detection.get_key_point(
                    detection,
                    mp_face_detection.FaceKeyPoint.NOSE_TIP)
                nose_tips.append([nose_tip.x, nose_tip.y])
                landmarks.landmarks[FacialLandmarks.NOSE] = \
                    PointOfInterest2D(
                        int(min(nose_tip.x*img_width, img_width)),
                        int(min(nose_tip.y*img_height, img_height)),
                        1)
                right_eye = mp_face_detection.get_key_point(
                    detection,
                    mp_face_detection.FaceKeyPoint.RIGHT_EYE)
                right_eyes.append([right_eye.x, right_eye.y])
                landmarks.landmarks[FacialLandmarks.RIGHT_PUPIL] = \
                    PointOfInterest2D(
                        int(min(right_eye.x*img_width, img_width)),
                        int(min(right_eye.y*img_height, img_height)),
                        1)
                left_eye = mp_face_detection.get_key_point(
                    detection,
                    mp_face_detection.FaceKeyPoint.LEFT_EYE)
                left_eyes.append([left_eye.x, left_eye.y])
                landmarks.landmarks[FacialLandmarks.LEFT_PUPIL] = \
                    PointOfInterest2D(
                        int(min(left_eye.x*img_width, img_width)),
                        int(min(left_eye.y*img_height, img_height)),
                        1)
                mouth_center = mp_face_detection.get_key_point(
                    detection,
                    mp_face_detection.FaceKeyPoint.MOUTH_CENTER)
                mouth_centers.append([mouth_center.x, mouth_center.y])
                landmarks.landmarks[FacialLandmarks.MOUTH_INNER_TOP_2] = \
                    PointOfInterest2D(
                        int(min(mouth_center.x*img_width, img_width)),
                        int(min(mouth_center.y*img_height, img_height)),
                        1)
                right_ear_tragion = mp_face_detection.get_key_point(
                    detection,
                    mp_face_detection.FaceKeyPoint.RIGHT_EAR_TRAGION)
                right_ear_tragions.append(
                    [right_ear_tragion.x, right_ear_tragion.y])
                landmarks.landmarks[FacialLandmarks.RIGHT_EAR] = \
                    PointOfInterest2D(
                        int(min(right_ear_tragion.x*img_width, img_width)),
                        int(min(right_ear_tragion.y*img_height, img_height)),
                        1)
                left_ear_tragion = mp_face_detection.get_key_point(
                    detection,
                    mp_face_detection.FaceKeyPoint.LEFT_EAR_TRAGION)
                left_ear_tragions.append(
                    [left_ear_tragion.x, left_ear_tragion.y])
                landmarks.landmarks[FacialLandmarks.LEFT_EAR] = \
                    PointOfInterest2D(
                        int(min(left_ear_tragion.x*img_width, img_width)),
                        int(min(left_ear_tragion.y*img_height, img_height)),
                        1)
                facial_landmarks_msg.append(landmarks)

        results.append(nose_tips)
        results.append(right_eyes)
        results.append(left_eyes)
        results.append(mouth_centers)
        results.append(right_ear_tragions)
        results.append(left_ear_tragions)
        results.append(facial_landmarks_msg)

        return results

    def detect_face_mesh(self, img, img_height, img_width):
        """img is expected as RGB"""
        img_rows, img_cols, _ = img.shape

        detections = self.detector.process(img)
        results = []
        bbs = []
        nose_tips = []
        right_eyes = []
        left_eyes = []
        mouth_centers = []
        right_ear_tragions = []
        left_ear_tragions = []
        facial_landmarks_msg = []

        if detections and detections.multi_face_landmarks:
            for detection in detections.multi_face_landmarks:
                x_min = 1
                y_min = 1
                x_max = 0
                y_max = 0
                for idx, landmark in enumerate(detection.landmark):
                    if landmark.x < x_min:
                        x_min = landmark.x
                    if landmark.y < y_min:
                        y_min = landmark.y
                    if landmark.x > x_max:
                        x_max = landmark.x
                    if landmark.y > y_max:
                        y_max = landmark.y
                    if idx == 1:
                        nose_tips.append([landmark.x, landmark.y])
                    if idx == 13:
                        mouth_centers.append([landmark.x, landmark.y])
                    if idx == 159:
                        right_eyes.append([landmark.x, landmark.y])
                    if idx == 234:
                        right_ear_tragions.append([landmark.x, landmark.y])
                    if idx == 386:
                        left_eyes.append([landmark.x, landmark.y])
                    if idx == 454:
                        left_ear_tragions.append([landmark.x, landmark.y])

                x, y = normalized_to_pixel_coordinates(
                    x_min, y_min, img_cols, img_rows
                )
                w, h = normalized_to_pixel_coordinates(
                    x_max-x_min, y_max-y_min, img_cols, img_rows
                )
                bbs.append((x, y, w, h))
                facial_landmarks_msg.append(
                    self.make_facial_landmarks_msg(
                        detection,
                        img_height,
                        img_width))

        results.append(bbs)
        results.append(nose_tips)
        results.append(right_eyes)
        results.append(left_eyes)
        results.append(mouth_centers)
        results.append(right_ear_tragions)
        results.append(left_ear_tragions)
        results.append(facial_landmarks_msg)

        return results

    def get_boundingboxes(self, detections, image_cols, image_rows):
        """
        Based on https://github.com/google/mediapipe/blob/master/mediapipe/python/solutions/drawing_utils.py
        """
        res = []
        if not detections:
            return res

        for detection in detections:
            bb = detection.location_data.relative_bounding_box
            x, y = normalized_to_pixel_coordinates(
                bb.xmin, bb.ymin, image_cols, image_rows
            )
            w, h = normalized_to_pixel_coordinates(
                bb.width, bb.height, image_cols, image_rows
            )

            res.append((x, y, w, h))

        return res

    def __str__(self):
        return "Google mediapipe face detector"


class RosFaceDetector:
    def __init__(self, debug=False, preallocate=False, face_mesh=True, max_num_faces=4):

        self.is_shutting_down = False
        self.debug = debug
        self.face_mesh = face_mesh

        semaphore_pub = rospy.Publisher(
            "/hri_face_detect/ready", Empty, queue_size=1, latch=True
        )
        self.faces_pub = rospy.Publisher("/humans/faces/tracked", IdsList, queue_size=1)

        self.facedetector = FaceDetector(face_mesh, max_num_faces)
        self.image_sub = rospy.Subscriber(
            "image", Image, partial(self.callback, preallocate)
        )
        self.image_info_sub = rospy.Subscriber(
            "camera_info", CameraInfo, self.info_callback
        )

        # holds all the individual face publishers
        # only used when pre-allocating face publishers
        if preallocate:
            self.face_pubs = {
                "%05d"
                % i: (
                    rospy.Publisher(
                        "/humans/faces/%05d/roi" % i,
                        RegionOfInterest,
                        queue_size=1
                    ),
                    rospy.Publisher(
                        "/humans/faces/%05d/cropped" % i,
                        Image,
                        queue_size=1
                    ),
                )
                for i in range(1, 30)
            }
        else:
            self.face_pubs = []

        rospy.loginfo(
            "Ready. Waiting for images to be published on %s."
            % self.image_sub.name
        )
        semaphore_pub.publish(Empty())

        # last-used face id
        self.last_id = 1

        # ID -> (publisher, RoI, nb_frames_visible)
        self.detectedFaces = {}

        self.tb = tf.TransformBroadcaster()

    def find_previous_match(self, bb):
        for id, value in self.detectedFaces.items():
            _, prev_bb, _, _ = value
            if (
                self.distance_rois(
                    prev_bb, bb) < MAX_ROIS_DISTANCE * MAX_ROIS_DISTANCE
                and 1/MAX_SCALING_ROIS < prev_bb.width/bb.width < MAX_SCALING_ROIS
                and 1/MAX_SCALING_ROIS < prev_bb.height/bb.height < MAX_SCALING_ROIS
            ):
                return id
        return None

    def publish_cropped_faces(self, src_image, bbs_and_publishers):

        for id, kv in bbs_and_publishers.items():
            pubs, bb, _, _ = kv

            if not pubs:
                continue

            # no-one interested in the face image? skip it!
            if pubs[1].get_num_connections() == 0 and not self.debug:
                continue

            roi = src_image[
                bb.y_offset: bb.y_offset + bb.height,
                bb.x_offset: bb.x_offset + bb.width,
            ]

            sx = cropped_face_width * 1.0 / bb.width
            sy = cropped_face_height * 1.0 / bb.height

            scale = min(sx, sy)

            scaled = cv2.resize(roi, None, fx=scale, fy=scale)
            scaled_h, scaled_w = scaled.shape[:2]

            output = np.zeros(
                (cropped_face_width, cropped_face_height, 3), np.uint8)

            x_offset = int((cropped_face_width - scaled_w) / 2)
            y_offset = int((cropped_face_height - scaled_h) / 2)

            output[
                y_offset: y_offset + scaled_h, x_offset: x_offset + scaled_w
            ] = scaled

            if self.debug:

                cv2.imshow("Face %s" % id, output)

            pubs[1].publish(CvBridge().cv2_to_imgmsg(output, encoding="bgr8"))

    def info_callback(self, cameraInfo):

        if not hasattr(self, 'cameraInfo'):
            self.cameraInfo = cameraInfo

            self.K = np.zeros((3, 3), np.float32)
            self.K[0][0:3] = self.cameraInfo.K[0:3]
            self.K[1][0:3] = self.cameraInfo.K[3:6]
            self.K[2][0:3] = self.cameraInfo.K[6:9]

    def callback(self, preallocate, rgb_msg):

        if self.is_shutting_down:
            return

        image = CvBridge().imgmsg_to_cv2(rgb_msg, desired_encoding="bgr8")
        img_height, img_width, _ = image.shape
        camera_optical_frame = rgb_msg.header.frame_id

        if not self.face_mesh:
            detections = self.facedetector.detect(image, img_width, img_height)
        else:
            detections = self.facedetector.detect_face_mesh(
                image,
                img_height,
                img_width
            )
        bbs = detections[0]
        nose_tips = detections[1]
        right_eyes = detections[2]
        left_eyes = detections[3]
        mouth_centers = detections[4]
        right_ear_tragions = detections[5]
        left_ear_tragions = detections[6]
        facial_landmarks_msg = detections[7]

        currentFaces = {}
        for i, bb in enumerate(bbs):
            x, y, w, h = bb
            bb = RegionOfInterest(
                max(0, x),
                max(0, y),
                min(img_height - y, h),
                min(img_width - x, w),
                True,
            )

            landmarks = facial_landmarks_msg[i]

            id = self.find_previous_match(bb)
            if id:
                # we re-detect a face
                # if it is a 2nd frame, we create a publisher for it.
                if not self.detectedFaces[id][0]:
                    want_deterministic_id = preallocate
                    final_id = self.generate_face_id(want_deterministic_id)
                    rospy.loginfo("New face [face_%s]" % final_id)

                    if preallocate:
                        pubs = self.face_pubs[final_id]
                    else:
                        pubs = (
                            rospy.Publisher(
                                "/humans/faces/%s/roi" % final_id,
                                RegionOfInterest,
                                queue_size=1,
                            ),
                            rospy.Publisher(
                                "/humans/faces/%s/cropped" % final_id,
                                Image,
                                queue_size=1,
                            ),
                            rospy.Publisher("/humans/faces/%s/landmarks"
                                            % final_id,
                                            FacialLandmarks,
                                            queue_size=1,
                                            )
                        )

                    currentFaces[final_id] = (
                        pubs, bb, self.detectedFaces[id][2] + 1, landmarks)
                else:
                    currentFaces[id] = (
                        self.detectedFaces[id][0],
                        bb,
                        self.detectedFaces[id][2] + 1,
                        landmarks
                    )
                    ### Face pose estimation ###
                    if hasattr(self, 'K'):
                        points_2D = np.array([
                            normalized_to_pixel_coordinates(
                                nose_tips[i][0],
                                nose_tips[i][1],
                                img_width,
                                img_height),
                            normalized_to_pixel_coordinates(
                                right_eyes[i][0],
                                right_eyes[i][1],
                                img_width,
                                img_height),
                            normalized_to_pixel_coordinates(
                                left_eyes[i][0],
                                left_eyes[i][1],
                                img_width,
                                img_height),
                            normalized_to_pixel_coordinates(
                                mouth_centers[i][0],
                                mouth_centers[i][1],
                                img_width,
                                img_height),
                            normalized_to_pixel_coordinates(
                                right_ear_tragions[i][0],
                                right_ear_tragions[i][1],
                                img_width,
                                img_height),
                            normalized_to_pixel_coordinates(
                                left_ear_tragions[i][0],
                                left_ear_tragions[i][1],
                                img_width,
                                img_height)
                        ], dtype="double")

                        success, rot_vec, trans_vec = cv2.solvePnP(
                            points_3D,
                            points_2D,
                            self.K,
                            None,
                            tvec=np.array([0., 0., 1000.]),
                            useExtrinsicGuess=True,
                            flags=4)

                    # calculating angle
                        rmat, jac = cv2.Rodrigues(rot_vec)
                        angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)
                        self.tb.sendTransform(
                            (trans_vec[0]/1000,
                             trans_vec[1]/1000,
                             trans_vec[2]/1000),
                            tf.transformations.quaternion_from_euler(
                                angles[0]/180*np.pi,
                                angles[1]/180*np.pi,
                                angles[2]/180*np.pi),
                            rospy.Time.now(),
                            "face_"+id,
                            camera_optical_frame)
                        self.tb.sendTransform(
                            (0, 0, 0),
                            tf.transformations.quaternion_from_euler(
                                -np.pi/2,
                                0,
                                -np.pi/2),
                            rospy.Time.now(),
                            "gaze_"+id,
                            "face_"+id)

            else:
                # we 'provisionally' store the face - we'll create a publisher
                # and start publishing only if we see the
                # face a second time
                id = self.generate_tmp_face_id()
                currentFaces[id] = (None, bb, 1, landmarks)

        # iterate over faces not seen anymore,
        # and unregister corresponding publishers
        for id, value in self.detectedFaces.items():
            if id not in currentFaces:
                pubs, _, nb_frames, _ = value
                if pubs:
                    rospy.loginfo(
                        "Face [face_%s] lost. It remained visible for %s frames"
                        % (id, nb_frames)
                    )
                    for pub in pubs:
                        pub.unregister()

        self.detectedFaces = currentFaces

        list_ids = []

        for id, value in self.detectedFaces.items():
            pubs, bb, _, landmarks = value
            if pubs and not self.is_shutting_down:
                list_ids.append(str(id))
                pubs[0].publish(bb)
                pubs[2].publish(landmarks)

        if not self.is_shutting_down:
            self.publish_cropped_faces(image, self.detectedFaces)
        if not self.is_shutting_down:
            self.faces_pub.publish(IdsList(rgb_msg.header, list_ids))

        if self.debug:
            # Draw the face detection annotations on the image.
            image.flags.writeable = True
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
            rospy.loginfo("%s faces detected" % len(self.detectedFaces))
            for id, value in self.detectedFaces.items():
                pub, bb, nb_frames = value
                if not pub:
                    continue
                cv2.rectangle(
                    image,
                    (bb.x_offset, bb.y_offset),
                    (bb.x_offset + bb.width, bb.y_offset + bb.height),
                    (255, 255, 0),
                    2,
                )
                cv2.putText(
                    image,
                    id,
                    (bb.x_offset, bb.y_offset),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1,
                    (255, 0, 0),
                )

            cv2.imshow("MediaPipe Face Detection", image)
            cv2.waitKey(5)

    def distance_rois(self, bb1, bb2):
        x1, y1 = bb1.x_offset + bb1.width / 2, bb1.y_offset + bb1.height / 2
        x2, y2 = bb2.x_offset + bb2.width / 2, bb2.y_offset + bb2.height / 2

        return (x1 - x2) * (x1 - x2) + (y1 - y2) * (y1 - y2)

    def generate_face_id(self, deterministic=False):
        if deterministic:
            id_str = "%05d" % self.last_id
            self.last_id = (self.last_id + 1) % 10000
            return id_str

        else:
            return str(uuid.uuid4())[:5]  # for a 5 char long ID

    def generate_tmp_face_id(self):
        return str(uuid.uuid4())[:5]  # for a 5 char long ID

    def close(self):

        rospy.loginfo("Stopping face publishing...")

        self.is_shutting_down = True

        for id, value in self.detectedFaces.items():
            pubs, _, _, _ = value
            for pub in pubs:
                pub.unregister()

        h = Header()
        h.stamp = rospy.Time.now()
        self.faces_pub.publish(IdsList(h, []))

        rospy.loginfo("Stopped publishing faces.")

        rospy.sleep(
            0.1
        )  # ensure the last messages published in detector.close() are effectively sent.


if __name__ == "__main__":
    rospy.init_node("hri_face_detect")

    cropped_face_width = rospy.get_param("/humans/faces/width", cropped_face_width)
    cropped_face_height = rospy.get_param("/humans/faces/height", cropped_face_height)

    debug = rospy.get_param("~debug", False)

    # if set to true, face IDs will be generated as a sequence of integers,
    # starting at 00001, and ROS publishers for each face are created *before*
    # the faces are detected. This makes it possible for client to subscribe
    # early to these topics, to avoid missing any message. This is useful to eg
    # annotate datasets.

    # if false, face IDs will be a random set of 5 characters in [0-9a-f], and
    # face subtopics will be created on the fly.
    preallocate = rospy.get_param("~preallocate_topics", False)
    face_mesh = rospy.get_param("~face_mesh", True)
    max_num_faces = rospy.get_param("~max_num_faces", 4)

    detector = RosFaceDetector(debug, preallocate, face_mesh, max_num_faces)

    rospy.on_shutdown(detector.close)

    rospy.spin()
